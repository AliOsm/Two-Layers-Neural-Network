{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Packages\n",
    "\n",
    "- [numpy](www.numpy.org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(961)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Load the dataset\n",
    "\n",
    "Load the [dataset](https://archive.ics.uci.edu/ml/datasets/banknote+authentication#) from `data.csv` file and split it to train (60%) and test (40%) sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('data.csv', delimiter=',')\n",
    "np.random.shuffle(data)\n",
    "m = data.shape[0]\n",
    "\n",
    "X_train = data[:int(m * 0.6), 0:4]\n",
    "X_train = X_train.T\n",
    "Y_train = data[:int(m * 0.6), 4:]\n",
    "Y_train = Y_train.T\n",
    "X_test = data[int(m * 0.6):, 0:4]\n",
    "X_test = X_test.T\n",
    "Y_test = data[int(m * 0.6):, 4:]\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Activation functions\n",
    "\n",
    "- [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function)\n",
    "- [relu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Z: the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    A: the output of the activation function\n",
    "    cache: dictionary containing Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A = (1.0 / (1.0 + np.exp(-Z)))\n",
    "    cache = (Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Z: the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ: gradients of the activations\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    s = 1.0 / (1.0 + np.exp(-Z))\n",
    "    dZ = dA * s * (1 - s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Z: the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    A: the output of the activation function\n",
    "    cache: dictionary containing Z\n",
    "    \"\"\"\n",
    "    \n",
    "    A = Z * (Z > 0)\n",
    "    cache = (Z)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Z: the input for the activation function\n",
    "    \n",
    "    Returns:\n",
    "    dZ: gradients of the activations\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = dA * (Z > 0)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Initialization\n",
    "\n",
    "Initialize the weights and biases matrices and vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    n_x: number of units in the input layer\n",
    "    n_h: number of units in the hidden layer\n",
    "    n_y: number of units in the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters: dictionary containing the parameters\n",
    "                W1, W2: weights\n",
    "                b1, b2: biases\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = np.random.randn(n_h, n_x) * 0.01\n",
    "    b1 = np.random.randn(n_h, 1)\n",
    "    W2 = np.random.randn(n_y, n_h) * 0.01\n",
    "    b2 = np.random.randn(n_y, 1)\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    A: activations from the previous layer\n",
    "    W: weights matrix\n",
    "    b: biases vector\n",
    "    \n",
    "    Returns:\n",
    "    Z: the input for the next activation function\n",
    "    cache: dictionary containing A, W and b; stored to compute backward propagation step\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = np.dot(W, A) + b\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    A_prev: activation from the previous layer\n",
    "    W: weights matrix\n",
    "    b: biases vector\n",
    "    activation: activation function to use\n",
    "    \n",
    "    Returns:\n",
    "    A: output of the activation function\n",
    "    cache: dictionary stored linear_cache and activation_cache to compute backward propagation step\n",
    "    \"\"\"\n",
    "    \n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "    \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Cost function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(Yhat, Y):\n",
    "    \"\"\"\n",
    "    Yhat: probabilities vector\n",
    "    Y: labels vector\n",
    "    \n",
    "    Returns:\n",
    "    cost: cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    cost = -(1.0 / m) * (np.dot(Y, np.log(Yhat).T) + np.dot(1 - Y, np.log(1 - Yhat).T))\n",
    "    cost = np.squeeze(cost)\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    dZ: gradients of activations\n",
    "    cache: tuple (A_prev, W, b)\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradients of activations\n",
    "    dW: gradients of weights\n",
    "    db: gradients of biases\n",
    "    \"\"\"\n",
    "    \n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "    \n",
    "    dW = (1.0 / m) * np.dot(dZ, A_prev.T)\n",
    "    db = (1.0 / m) * np.sum(dZ, axis=1, keepdims=True)\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    dA: gradients of activations\n",
    "    cache: tuple (linear_cache, activation_cache)\n",
    "    activation: activation function to use\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev: gradients of activations\n",
    "    dW: gradients of weights\n",
    "    db: gradients of biases\n",
    "    \"\"\"\n",
    "    \n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Update parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    parameters: dictionary containing the parameters\n",
    "    grads: dictionary contaning the gradients\n",
    "    learning_rate: the learning rate\n",
    "    \n",
    "    Returns:\n",
    "    parameters: the updated parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2\n",
    "    \n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * grads[\"dW\" + str(l+1)]\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * grads[\"db\" + str(l+1)]\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9 - The model\n",
    "\n",
    "Everything come together here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000):\n",
    "    \"\"\"\n",
    "    X: training examples\n",
    "    Y: training labels\n",
    "    layers_dims: layers dimensions\n",
    "    learning_rate: the learning rate\n",
    "    num_iterations: number of iterations for gradient descent\n",
    "    \n",
    "    Returns:\n",
    "    parameters: the parameters for the final iteration\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    m = X.shape[1]\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    for i in range(0, num_iterations):\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "        \n",
    "        cost = compute_cost(A2, Y)\n",
    "        \n",
    "        dA2 = -(np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, \"relu\")\n",
    "        \n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i + 1, np.squeeze(cost)))\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 100: 0.714546164559489\n",
      "Cost after iteration 200: 0.6295192563324075\n",
      "Cost after iteration 300: 0.4817191360592121\n",
      "Cost after iteration 400: 0.3654151694209473\n",
      "Cost after iteration 500: 0.30879554582777397\n",
      "Cost after iteration 600: 0.2746478320170577\n",
      "Cost after iteration 700: 0.24942371507419836\n",
      "Cost after iteration 800: 0.22895475144693245\n",
      "Cost after iteration 900: 0.21167534592644754\n",
      "Cost after iteration 1000: 0.19680165642267408\n",
      "Cost after iteration 1100: 0.18389434516197503\n",
      "Cost after iteration 1200: 0.1725856156668431\n",
      "Cost after iteration 1300: 0.1625935396793638\n",
      "Cost after iteration 1400: 0.15367573372549234\n",
      "Cost after iteration 1500: 0.14570517078620554\n",
      "Cost after iteration 1600: 0.13854384604144165\n",
      "Cost after iteration 1700: 0.13207505249750243\n",
      "Cost after iteration 1800: 0.12620555556178253\n",
      "Cost after iteration 1900: 0.12087263753740299\n",
      "Cost after iteration 2000: 0.11602343644213224\n",
      "Cost after iteration 2100: 0.11158771362264092\n",
      "Cost after iteration 2200: 0.1075127840436194\n",
      "Cost after iteration 2300: 0.10375568478073217\n",
      "Cost after iteration 2400: 0.10028586943415911\n",
      "Cost after iteration 2500: 0.09707271324706092\n",
      "Cost after iteration 2600: 0.09408772453821909\n",
      "Cost after iteration 2700: 0.09131089706474799\n",
      "Cost after iteration 2800: 0.08872108520441521\n",
      "Cost after iteration 2900: 0.08630163606450796\n",
      "Cost after iteration 3000: 0.08403551181027433\n"
     ]
    }
   ],
   "source": [
    "parameters = model(X_train, Y_train, layers_dims = (4, 8, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10 - Prediction\n",
    "\n",
    "Use `X_test` and `Y_test` to make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, Y, parameters):\n",
    "    \"\"\"\n",
    "    X: test examples\n",
    "    Y: test labels\n",
    "    parameters: gradient descent parameters\n",
    "    \n",
    "    Returns:\n",
    "    percent: the percentage of correction\n",
    "    \"\"\"\n",
    "    \n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    A1, cache = linear_activation_forward(X, W1, b1, \"relu\")\n",
    "    A2, cache = linear_activation_forward(A1, W2, b2, \"sigmoid\")\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "    predictions = (Y == (A2 > 0.5))\n",
    "    percent = np.sum(predictions) / m * 100\n",
    "    \n",
    "    return percent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "98.91%\n"
     ]
    }
   ],
   "source": [
    "print(\"{0:.2f}%\".format(predict(X_test, Y_test, parameters)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
